{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from config import START,END,INTERVAL,INIT_H,END_H,SATELLITE,PRODUCT,CHANNEL,TMP,OUTPUT\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from datetime import *\n",
    "import pathlib\n",
    "\n",
    "import netCDF4\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_list():\n",
    "    days = pd.date_range(start=START, end=END, freq=INTERVAL)\n",
    "    hours = pd.date_range(INIT_H,END_H, freq=INTERVAL).strftime('%H:%M:%S')\n",
    "\n",
    "    data_range = []\n",
    "    for d in range(len(days)):\n",
    "        for m in range(len(hours)):\n",
    "            if days[d].strftime('%H:%M:%S') == hours[m]:\n",
    "                data_range.append(days[d])\n",
    "    return data_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aws_file_list(list_of_files):\n",
    "\n",
    "    DownloadList = pd.DataFrame()  \n",
    "    timestamp = []\n",
    "    fileslist = []\n",
    "    stringList = []\n",
    "    \n",
    "    for i in list_of_files:\n",
    "        ListFiles = np.array(aws.ls(server+i.strftime('%Y/%j/%H')))\n",
    "        stringList.append(i.strftime('%Y%j%H%M'))\n",
    "        timestamp.append(i.strftime('%Y/%m/%d %H:%M'))\n",
    "        \n",
    "        for file in ListFiles: \n",
    "            if file.find('M3C'+str(CHANNEL)) >= 1:\n",
    "                fileslist.append(str(file))\n",
    "                \n",
    "            elif file.find('M6C'+str(CHANNEL)) >= 1:\n",
    "                fileslist.append(str(file))\n",
    "                \n",
    "                \n",
    "    list_ = pd.DataFrame({'url':fileslist})\n",
    "    list_ = list_.drop_duplicates()\n",
    "\n",
    "    regstr = '|'.join(stringList)\n",
    "    list_['strings'] = list_['url'].str.upper().str.contains(regstr)\n",
    "\n",
    "    DownloadList = list_[list_.strings]\n",
    "    DownloadList = DownloadList.drop(['strings'], axis=1)\n",
    "    DownloadList['timestamp'] = timestamp\n",
    "    DownloadList['path'] = DownloadList['url'].str.replace(SATELLITE, TMP)\n",
    "    DownloadList['url'] = DownloadList['url'].str.replace(SATELLITE, 'https://'+SATELLITE+'.s3.amazonaws.com')\n",
    "    DownloadList['file'] = DownloadList.apply(lambda x: pathlib.Path(x.path).name, axis=1)\n",
    "    DownloadList['path'] = DownloadList.apply(lambda x: pathlib.Path(x.path).parent, axis=1)\n",
    "\n",
    "    return DownloadList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(list_of_files):\n",
    "\n",
    "    fils = len(list_of_files)\n",
    "    cnt = 0\n",
    "    \n",
    "    for i,row in list_of_files.iterrows():\n",
    "        print('File Counter',cnt+1,'/',fils)\n",
    "        print('Downloading...')\n",
    "        dat_time = datetime.strptime(row.timestamp, '%Y/%m/%d %H:%M')\n",
    "        path = str(dat_time.year)+'/'+str(dat_time.strftime('%m'))+'/'+str(dat_time.strftime('%d'))\n",
    "        output = OUTPUT+'/'+path\n",
    "\n",
    "        StartTime = datetime.now()\n",
    "        req = requests.get(row.url, stream = True)\n",
    "        total_size = int(req.headers['content-length'])\n",
    "        size = 0\n",
    "        \n",
    "        pathlib.Path(row.path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(str(row.path)+'/'+str(row.file),'wb') as output:\n",
    "            for chunk in req.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    rec_size = output.write(chunk)\n",
    "                    size = rec_size + size\n",
    "                    print('{}\\t{:3.0f}%\\t{:.2f} min'.format(row.file,100.0*size/total_size, (datetime.now()-StartTime).seconds/60.0), end='\\r', flush=True)\n",
    "        print('\\n')\n",
    "        \n",
    "        # Process\n",
    "        print('Processing...')\n",
    "        path_ = (str(row.path)+'/'+str(row.file))\n",
    "        open_netcdf(path_,row.file,output)\n",
    "        cnt +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_netcdf(path_,file,output):\n",
    "    ## Copy global atributes\n",
    "    dataset = netCDF4.Dataset(path_, 'a')\n",
    "    scale_factor = dataset.variables['CMI'].scale_factor\n",
    "    long_name = dataset.variables['CMI'].long_name\n",
    "    standard_name = dataset.variables['CMI'].standard_name\n",
    "    sensor_band_bit_depth = dataset.variables['CMI'].sensor_band_bit_depth\n",
    "    valid_range = dataset.variables['CMI'].valid_range\n",
    "    add_offset = dataset.variables['CMI'].add_offset\n",
    "    units = dataset.variables['CMI'].units\n",
    "    resolution = dataset.variables['CMI'].resolution\n",
    "    grid_mapping = dataset.variables['CMI'].grid_mapping\n",
    "    cell_methods = dataset.variables['CMI'].cell_methods\n",
    "    ## Atributes\n",
    "    naming_authority = dataset.naming_authority\n",
    "    inst_ = dataset.institution\n",
    "    project = dataset.project\n",
    "    production_site = dataset.production_site\n",
    "    production_environment = dataset.production_environment\n",
    "    spatial_resolution = dataset.spatial_resolution\n",
    "    orbital_slot = dataset.orbital_slot\n",
    "    platform_ID = dataset.platform_ID\n",
    "    instrument_type = dataset.instrument_type\n",
    "    scene_id = dataset.scene_id\n",
    "    instrument_ID = dataset.instrument_ID\n",
    "    dataset_name = dataset.dataset_name\n",
    "    title = dataset.title\n",
    "    summary = dataset.summary\n",
    "    keywords = dataset.keywords\n",
    "    keywords_vocabulary = dataset.keywords_vocabulary\n",
    "    license = dataset.license\n",
    "    processing_level = dataset.processing_level\n",
    "    date_created = dataset.date_created\n",
    "    cdm_data_type = dataset.cdm_data_type\n",
    "    time_coverage_start = dataset.time_coverage_start\n",
    "    time_coverage_end = dataset.time_coverage_end\n",
    "    timeline_id = dataset.timeline_id\n",
    "    production_data_source = dataset.production_data_source\n",
    "    id__ = dataset.id\n",
    "    \n",
    "    ### Translate\n",
    "    print('Translating file...')\n",
    "    cmd1 = \"gdal_translate -q -a_srs\"\n",
    "    cmd2 = \" \\\"+proj=geos +a=6.37814e+06  +b=6.35675e+06 +lon_0=-75 +f=298.257 +h=35786023 +sweep=x\\\" \"\n",
    "    cmd3 = \"-a_scale \"+str(scale_factor)+\" -a_ullr -5434390.3880000000000 5434390.3880000000000 5434390.3880000000000 -5434390.3880000000000 HDF5:\"\n",
    "    cmd4 = \"\\\"./\"+str(path_)+\"\\\"\"\n",
    "    cmd5 = \"://CMI -a_nodata -1 -of netCDF temp/navigation.modified.nc\"\n",
    "    full_cmd = cmd1+cmd2+cmd3+cmd4+cmd5\n",
    "\n",
    "    os.environ['HDF5_USE_FILE_LOCKING']='FALSE'\n",
    "    os.system(full_cmd)\n",
    "    \n",
    "    ## Warp file\n",
    "    print('Warping file...')\n",
    "    cmd1_ = \"gdalwarp -q -multi \"\n",
    "    cmd2_ = \"-s_srs \\\"+proj=geos +a=6.37814e+06  +b=6.35675e+06 +lon_0=-75 +f=298.257 +h=35786023\\\" \"\n",
    "    cmd3_ = \"-nomd -te -75.0 -35.0 -33.0 7.0 -t_srs '+proj=latlong +datum=WGS84\\' \"\n",
    "    cmd4_ = \"./temp/navigation.modified.nc \"\n",
    "    cmd5_ = \"-of netCDF -co compress=DEFLATE -co FORMAT=NC4C -r near \"\n",
    "    cmd6_ = output\n",
    "    \n",
    "    full_cmd_ = (cmd1_+cmd2_+cmd3_+cmd4_+cmd5_+cmd6_)\n",
    "\n",
    "    os.system(full_cmd_)\n",
    "    ## Rename Band 1 to CMI\n",
    "    os.system(\"ncrename -h -O -v Band1,CMI temp/\"+str(output))\n",
    "    ## Rename longname\n",
    "    os.system(\"ncatted -O -a long_name,CMI,o,c,\\\"\"+str(long_name)+\"\\\" \"+str(output))\n",
    "    os.system(\"ncatted -O -a standard_name,CMI,o,c,\\\"\"+str(standard_name)+\"\\\" \"+str(output))\n",
    "    os.system(\"ncatted -O -a scale_factor,CMI,o,f,\\\"\"+str(scale_factor)+\"\\\" \"+str(output))\n",
    "    os.system(\"ncatted -O -a sensor_band_bit_depth,CMI,o,b,\\\"\"+str(sensor_band_bit_depth)+\"\\\" \"+str(output))\n",
    "    os.system(\"ncatted -O -a valid_range,CMI,o,s,\\\"\"+str(valid_range)+\"\\\" \"+str(output))\n",
    "    os.system(\"ncatted -O -a add_offset,CMI,o,f,\\\"\"+str(add_offset)+\"\\\" \"+str(output))\n",
    "    os.system(\"ncatted -O -a units,CMI,o,c,\\\"\"+str(units)+\"\\\" \"+str(output))\n",
    "    os.system(\"ncatted -O -a resolution,CMI,o,c,\\\"\"+str(resolution)+\"\\\" \"+str(output))\n",
    "    os.system(\"ncatted -O -a grid_mapping,CMI,o,c,\\\"\"+str(grid_mapping)+\"\\\" \"+str(output))\n",
    "    os.system(\"ncatted -O -a cell_methods,CMI,o,c,\\\"\"+str(cell_methods)+\"\\\" \"+str(output))\n",
    "    ## Add Global atributes \n",
    "    os.system(\"ncatted -O -h -a naming_authority,global,o,c,\"+str(naming_authority)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a institution,global,o,c,\"+str(inst_)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a clipping,global,o,c,INPE LABREN Laboratorio de Modelagem e Estudos de Recursos Renovaveis de Energia \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a reprojection,global,o,c,WGS84 \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a project,global,o,c,\"+str(project)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a production_site,global,o,c,\"+str(production_site)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a production_environment,global,o,c,\"+str(production_environment)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a spatial_resolution,global,o,c,\"+str(spatial_resolution)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a orbital_slot,global,o,c,\"+str(spatial_resolution)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a platform_ID,global,o,c,\"+str(platform_ID)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a instrument_type,global,o,c,\"+str(instrument_type)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a scene_id,global,o,c,\"+str(scene_id)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a instrument_ID,global,o,c,\"+str(instrument_ID)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a dataset_name,global,o,c,\"+str(dataset_name)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a title,global,o,c,\"+str(title)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a summary,global,o,c,\"+str(summary)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a keywords,global,o,c,\"+str(keywords)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a keywords_vocabulary,global,o,c,\"+str(keywords_vocabulary)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a license,global,o,c,\"+str(license)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a processing_level,global,o,c,\"+str(processing_level)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a date_created,global,o,c,\"+str(date_created)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a cdm_data_type,global,o,c,\"+str(cdm_data_type)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a time_coverage_start,global,o,c,\"+str(time_coverage_start)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a time_coverage_end,global,o,c,\"+str(time_coverage_end)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a timeline_id,global,o,c,\"+str(timeline_id)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a production_data_source,global,o,c,\"+str(production_data_source)+\" \"+str(output))\n",
    "    os.system(\"ncatted -O -h -a id,global,o,c,\"+str(id__)+\" \"+str(output))\n",
    "\n",
    "    \n",
    "    ## Remove navigation file\n",
    "    os.system(\"rm -rf temp/navigation.modified.nc\")\n",
    "    dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Counter 1 / 3\n",
      "Downloading...\n",
      "Processing...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'tmp/ABI-L2-CMIPF/2019/001/08/OR_ABI-L2-CMIPF-M3C02_G16_s20190010800366_e20190010811133_c20190010811203.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d011c3889a6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlista\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maws_file_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdownload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlista\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-ba77ea63aee6>\u001b[0m in \u001b[0;36mdownload_files\u001b[0;34m(list_of_files)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Processing...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mpath_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mopen_netcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mcnt\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-ffb5d87048bb>\u001b[0m in \u001b[0;36mopen_netcdf\u001b[0;34m(path_, file, output)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_netcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m## Copy global atributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetCDF4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mscale_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CMI'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlong_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CMI'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'tmp/ABI-L2-CMIPF/2019/001/08/OR_ABI-L2-CMIPF-M3C02_G16_s20190010800366_e20190010811133_c20190010811203.nc'"
     ]
    }
   ],
   "source": [
    "list_of_files = file_list()\n",
    "server = SATELLITE+'/'+PRODUCT+'/'    \n",
    "aws = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "lista = aws_file_list(list_of_files)\n",
    "download_files(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
